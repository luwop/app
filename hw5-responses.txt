5.1) I created a table in hive and hbase using the flight data. The quries i used
are below and the code changes I made are in app.js

CREATE TABLE grlewis_hw51_delay_by_origin_year
STORED AS ORC AS
SELECT
    f.year AS year,
    s.name AS airport_code,
    COUNT(*) AS total_flights,
    AVG(clear_delay) AS clear_delays,
    AVG(fog_delay) AS fog_delays,
    AVG(rain_delay) AS rain_delays,
    AVG(snow_delay) AS snow_delays,
    AVG(hail_delay) AS hail_delays,
    AVG(thunder_delay) AS thunder_delays,
    AVG(tornado_delay) AS tornado_delays
FROM flights_and_weather f
JOIN stations s
  ON f.origin_code = s.code
GROUP BY f.year, s.name;

INSERT OVERWRITE TABLE grlewis_hw51_delay_by_origin_year_hb
SELECT
    concat_ws('_', CAST(year AS STRING), airport_code) AS origin_year,
    map(
        'clear_delays', clear_delays,
        'fog_delays', fog_delays,
        'rain_delays', rain_delays,
        'snow_delays', snow_delays,
        'hail_delays', hail_delays,
        'thunder_delays', thunder_delays,
        'tornado_delays', tornado_delays
    )
FROM grlewis_hw51_delay_by_origin_year;

5.2)
You can make WordCount shorter by using countByValue instead of writing map and reduce steps.

val textFile = sc.textFile("/tmp/text/")
val words = textFile.flatMap(line => line.split(" "))
val counts = words.countByValue()

The countByValue action goes through all the words and counts how many times each one appears. 
It returns a map where each key is a word and each value is the count. 
You can print it with counts.foreach(println).

The good thing about countByValue is that it is easy to write and understand. It works well 
for small datasets and is quick for testing. The bad thing is that it does not work well with 
large datasets because it brings all the results to the driver, which can run out of memory. 
You also cannot do more Spark transformations after using it since it no longer returns an RDD.
I also think abstracting away the steps really takes away form the idea of using map and reduce :).
